# For this PGT, we've considered an SNO managed cluster with the following labels:
#   group-du-sno-zone: zone-1 
#   hardware-type: hw-type-platform-1
# ConfigMaps used:
#   group-hardware-types-configmap.yaml: group-hardware-types-configmap
#   group-zones-configmap.yaml:          group-zones-configmap
#   site-data-configmap.yaml:            site-data-configmap
apiVersion: policy.open-cluster-management.io/v1
kind: PolicyGenerator
metadata:
  name: group-du-sno-pg
placementBindingDefaults:
  name: group-du-sno-pb
policyDefaults:
  #categories: []
  #controls:
  #  - PR.DS-1 Data-at-rest
  namespace: ztp-group
  # Use an existing placement rule so that placement bindings can be consolidated.
  placement:
    labelSelector:
      group-du-sno-zone: "zone-1"
      hardware-type: "hw-type-platform-1"
  remediationAction: inform
  severity: low
  # standards: []
  namespaceSelector:
    exclude:
      - kube-*
    include:
      - '*'
  evaluationInterval:
    compliant: 10m
    noncompliant: 10s
policies:
- name: gr-du-sno-cfg-pc-templated
  policyAnnotations:
    ran.openshift.io/ztp-deploy-wave: "10"
  manifests:
    - path: source-crs/DisableOLMPprof.yaml
    - path: source-crs/ClusterLogForwarder.yaml
      patches:
      - spec:
          outputs: '{{hub fromConfigMap "" "group-zones-configmap" (printf "%s-cluster-log-fwd-outputs" (index .ManagedClusterLabels "group-du-sno-zone")) | toLiteral hub}}'
          pipelines: '{{hub fromConfigMap "" "group-zones-configmap" (printf "%s-cluster-log-fwd-pipelines" (index .ManagedClusterLabels "group-du-sno-zone")) | toLiteral hub}}'
    - path: source-crs/ClusterLogging.yaml
    ## The setting below overrides the default "worker" selector predefined in
    ## the source-crs. The change is recommended on SNOs configured with PTP 
    ## event notification for forward compatibility with possible SNO expansion.
    ## When the default setting is left intact, then in case of an SNO 
    ## expansion with one or more workers, PTP operator
    ## would not create linuxptp-daemon containers on the worker node(s). Any
    ## attempt to change the daemonsetNodeSelector will result in ptp daemon
    ## restart and time synchronization loss.
    ## After complying with the policy, complianceType can be set to a safer "musthave"
#    - path: source-crs/PtpOperatorConfigForEvent.yaml
#      complianceType: "mustonlyhave"
#      patches:
#      - spec:
#          daemonNodeSelector:
#            node-role.kubernetes.io/worker: ""
    - path: source-crs/PtpConfigSlave.yaml
      patches:
      - metadata:
          name: "du-ptp-slave" 
        spec:
          recommend:
          - match:
            - nodeLabel: node-role.kubernetes.io/master
            priority: 4
            profile: slave
          profile:
          - name: "slave"
            # This interface must match the hardware in this group
            interface: '{{hub fromConfigMap "" "group-hardware-types-configmap" (printf "%s-ptpcfgslave-profile-interface" (index .ManagedClusterLabels "hardware-type")) hub}}'
            ptp4lOpts: "-2 -s --summary_interval -4"
            phc2sysOpts: "-a -r -n 24"
            ptpSchedulingPolicy: SCHED_FIFO
            ptpSchedulingPriority: 10
            ptpSettings:
              logReduce: "true"
            ptp4lConf: |
              [global]
              #
              # Default Data Set
              #
              twoStepFlag 1
              slaveOnly 1
              priority1 128
              priority2 128
              domainNumber 24
              #utc_offset 37
              clockClass 255
              clockAccuracy 0xFE
              offsetScaledLogVariance 0xFFFF
              free_running 0
              freq_est_interval 1
              dscp_event 0
              dscp_general 0
              dataset_comparison G.8275.x
              G.8275.defaultDS.localPriority 128
              #
              # Port Data Set
              #
              logAnnounceInterval -3
              logSyncInterval -4
              logMinDelayReqInterval -4
              logMinPdelayReqInterval -4
              announceReceiptTimeout 3
              syncReceiptTimeout 0
              delayAsymmetry 0
              fault_reset_interval -4
              neighborPropDelayThresh 20000000
              masterOnly 0
              G.8275.portDS.localPriority 128
              #
              # Run time options
              #
              assume_two_step 0
              logging_level 6
              path_trace_enabled 0
              follow_up_info 0
              hybrid_e2e 0
              inhibit_multicast_service 0
              net_sync_monitor 0
              tc_spanning_tree 0
              tx_timestamp_timeout 50
              unicast_listen 0
              unicast_master_table 0
              unicast_req_duration 3600
              use_syslog 1
              verbose 0
              summary_interval 0
              kernel_leap 1
              check_fup_sync 0
              clock_class_threshold 7
              #
              # Servo Options
              #
              pi_proportional_const 0.0
              pi_integral_const 0.0
              pi_proportional_scale 0.0
              pi_proportional_exponent -0.3
              pi_proportional_norm_max 0.7
              pi_integral_scale 0.0
              pi_integral_exponent 0.4
              pi_integral_norm_max 0.3
              step_threshold 2.0
              first_step_threshold 0.00002
              max_frequency 900000000
              clock_servo pi
              sanity_freq_limit 200000000
              ntpshm_segment 0
              #
              # Transport options
              #
              transportSpecific 0x0
              ptp_dst_mac 01:1B:19:00:00:00
              p2p_dst_mac 01:80:C2:00:00:0E
              udp_ttl 1
              udp6_scope 0x0E
              uds_address /var/run/ptp4l
              #
              # Default interface options
              #
              clock_type OC
              network_transport L2
              delay_mechanism E2E
              time_stamping hardware
              tsproc_mode filter
              delay_filter moving_median
              delay_filter_length 10
              egressLatency 0
              ingressLatency 0
              boundary_clock_jbod 0
              #
              # Clock description
              #
              productDescription ;;
              revisionData ;;
              manufacturerIdentity 00:00:00
              userDescription ;
              timeSource 0xA0
    - path: source-crs/SriovOperatorConfig-SetSelector.yaml
      complianceType: musthave
      patches:
      - spec:
          configDaemonNodeSelector:
            node-role.kubernetes.io/worker: ""
    - path: source-crs/StorageLV.yaml
      patches:
      - spec:
          storageClassDevices:
          - storageClassName: "example-storage-class-1"
            volumeMode: Filesystem
            fsType: xfs
            devicePaths: '{{hub fromConfigMap "" "group-hardware-types-configmap" (printf "%s-storagelv-devicepaths-1" (index .ManagedClusterLabels "hardware-type")) | toLiteral hub}}'
          - storageClassName: "example-storage-class-2"
            volumeMode: Filesystem
            fsType: xfs
            devicePaths: '{{hub fromConfigMap "" "group-hardware-types-configmap" (printf "%s-storagelv-devicepaths-2" (index .ManagedClusterLabels "hardware-type")) | toLiteral hub}}'
       ## This is required if PTP and BMER operators use HTTP transport.
       ## The disk labels are created by ignitionConfigOverride in siteConfig.
       ## - storageClassName: "storage-class-http-events"
       ##   volumeMode: Filesystem
       ##   fsType: xfs
       ##   devicePaths:
       ##   - /dev/disk/by-partlabel/httpevent1
       ##   - /dev/disk/by-partlabel/httpevent2
    - path: source-crs/DisableSnoNetworkDiag.yaml
    - path: source-crs/PerformanceProfile-SetSelector.yaml
      patches:
      - metadata:
          name: openshift-node-performance-profile
        spec:
          additionalKernelArgs:
          - rcupdate.rcu_normal_after_boot=0
          - vfio_pci.enable_sriov=1
          - vfio_pci.disable_idle_d3=1
          - efi=runtime
          cpu:
            # These must be tailored for the specific hardware platform
            isolated: '{{hub fromConfigMap "" "group-hardware-types-configmap" (printf "%s-cpu-isolated" (index .ManagedClusterLabels "hardware-type")) hub}}'
            reserved: '{{hub fromConfigMap "" "group-hardware-types-configmap" (printf "%s-cpu-reserved" (index .ManagedClusterLabels "hardware-type")) hub}}'
          hugepages:
            defaultHugepagesSize: '{{hub fromConfigMap "" "group-hardware-types-configmap" (printf "%s-hugepages-default" (index .ManagedClusterLabels "hardware-type")) hub}}'
            pages:
              - size: '{{hub fromConfigMap "" "group-hardware-types-configmap" (printf "%s-hugepages-size" (index .ManagedClusterLabels "hardware-type")) hub}}'
                count: '{{hub fromConfigMap "" "group-hardware-types-configmap" (printf "%s-hugepages-count" (index .ManagedClusterLabels "hardware-type")) | toInt hub}}'
          realTimeKernel:
            enabled: true
          machineConfigPoolSelector:
            pools.operator.machineconfiguration.openshift.io/master: ""
          nodeSelector:
            node-role.kubernetes.io/master: ''
    - path: source-crs/TunedPerformancePatch.yaml # wave 10
      patches:
      - spec:
          recommend:
          - machineConfigLabels:
              machineconfiguration.openshift.io/role: "master"
            priority: 19
            profile: performance-patch
    - path: source-crs/optional-extra-manifest/enable-crun-master.yaml
    - path: source-crs/optional-extra-manifest/enable-crun-worker.yaml
#    - path: source-crs/AmqInstance.yaml
#    - path: source-crs/HardwareEvent.yaml
#      patches:
#      - spec:
#          nodeSelector: {}
#          logLevel: '{{hub fromConfigMap "" "group-zones-configmap" (printf "%s-hwevent-logLevel" (index .ManagedClusterLabels "group-du-sno-zone")) hub}}'
#    - path: source-crs/StorageClass.yaml
#      patches:
#      - metadata:
#          name: image-registry-sc # FIXED
#    - path: source-crs/StoragePVC.yaml # wave 10
#      patches:
#      - metadata:
#          name: image-registry-pvc # FIXED
#          namespace: openshift-image-registry
#        spec:
#          accessModes:
#            - ReadWriteMany
#          resources:
#            requests:
#              storage: '{{hub fromConfigMap "" "group-zones-configmap" (printf "%s-storagepvc-storage" (index .ManagedClusterLabels "group-du-sno-zone")) hub}}'
#          storageClassName: image-registry-sc # FIXED
#          volumeMode: Filesystem # FIXED
#    - path: source-crs/ImageRegistryPV.yaml
#    - path: source-crs/ImageRegistryConfig.yaml
#      patches:
#      - spec:
#          storage:
#            pvc:
#              claim: "image-registry-pvc" # FIXED
    - path: source-crs/SriovFecClusterConfig.yaml
      patches:
      - metadata:
          name: fec-config
        spec:
          drainSkip: true
          acceleratorSelector:
            pciAddress: '{{hub fromConfigMap "" "group-hardware-types-configmap" (printf "%s-sriov-fec-pciAddress" (index .ManagedClusterLabels "hardware-type")) hub}}'
          physicalFunction:
            pfDriver: '{{hub fromConfigMap "" "group-hardware-types-configmap" (printf "%s-sriov-fec-pfDriver" (index .ManagedClusterLabels "hardware-type")) hub}}'
            vfDriver: '{{hub fromConfigMap "" "group-hardware-types-configmap" (printf "%s-sriov-fec-vfDriver" (index .ManagedClusterLabels "hardware-type")) hub}}'
            vfAmount: 16
            $patch: replace
- name: gr-du-sno-sriov-pc-templated
  policyAnnotations:
    ran.openshift.io/ztp-deploy-wave: "10"
  manifests:
    - path: source-crs/SriovNetwork.yaml # wave 100
      patches:
      - metadata:
          name: sriov-nw-du-fh # FIXED
        spec:
          resourceName: du_fh # FIXED
          vlan: '{{hub fromConfigMap "" "site-data-configmap" (printf "%s-sriov-network-vlan-1" .ManagedClusterName) | toInt hub}}'
    - path: source-crs/SriovNetworkNodePolicy-SetSelector.yaml # wave 100
      patches:
      - metadata:
          name: sriov-nnp-du-fh
        spec:
          deviceType: netdevice # FIXED
          isRdma: false
          nicSelector:
            pfNames: '{{hub fromConfigMap "" "group-hardware-types-configmap" (printf "%s-sriov-node-policy-pfNames-1" (index .ManagedClusterLabels "hardware-type")) | toLiteral hub}}'
          numVfs: 8
          priority: 10
          resourceName: du_fh
          nodeSelector:
            node-role.kubernetes.io/master: ""
    - path: source-crs/SriovNetwork.yaml # wave 100
      patches:
      - metadata:
          name: sriov-nw-du-mh # FIXED
        spec:
          resourceName: du_mh # FIXED
          vlan: '{{hub fromConfigMap "" "site-data-configmap" (printf "%s-sriov-network-vlan-2" .ManagedClusterName) | toInt hub}}'
    - path: source-crs/SriovNetworkNodePolicy-SetSelector.yaml # wave 100
      patches:
      - metadata:
          name: sriov-nw-du-fh
        spec:
          deviceType: netdevice # FIXED
          isRdma: false
          nicSelector:
            pfNames: '{{hub fromConfigMap "" "group-hardware-types-configmap" (printf "%s-sriov-node-policy-pfNames-2" (index .ManagedClusterLabels "hardware-type")) | toLiteral hub}}'
          numVfs: 8
          priority: 10
          resourceName: du_fh
          nodeSelector:
            node-role.kubernetes.io/master: ""
#   --- sources needed for updating CRI-O workload-partitioning ----
#    - path: source-crs/MachineConfigGeneric.yaml
#      complianceType: mustonlyhave # This is to update array entry as opposed to appending a new entry.
#      patches:
#      - metadata:
#          name: 02-master-workload-partitioning
#        spec:
#          config:
#            storage:
#              files:
#                - contents:
#                    # crio cpuset config goes below. This value needs to be updated and matched with PerformanceProfile. Check the link for more info on the content.
#                    source: '{{hub fromConfigMap "" "group-hardware-types-configmap" (printf "%s-machine-config-storage-source-1" (index .ManagedClusterLabels "hardware-type")) hub}}'
#                  mode: 420
#                  overwrite: true
#                  path: /etc/crio/crio.conf.d/01-workload-partitioning
#                  user:
#                    name: root
#                - contents:
#                    # openshift cpuset config goes below. This value needs to be updated and matched with crio cpuset (array entry above this). Check the link for more info on the content.
#                    source: '{{hub fromConfigMap "" "group-hardware-types-configmap" (printf "%s-machine-config-storage-source-2" (index .ManagedClusterLabels "hardware-type")) hub}}'
#                  mode: 420
#                  overwrite: true
#                  path: /etc/kubernetes/openshift-workload-pinning
#                  user:
#                    name: root
