# For this PGT, we've considered an SNO managed cluster with the following labels:
#   group-du-sno-zone: zone-1 
#   hardware-type: hw-type-platform-1
# ConfigMaps used:
#   group-hardware-types-configmap.yaml: group-hardware-types-configmap
#   group-zones-configmap.yaml:          group-zones-configmap
#   site-data-configmap.yaml:            site-data-configmap
apiVersion: policy.open-cluster-management.io/v1
kind: PolicyGenerator
metadata:
  name: group-du-sno-pg
placementBindingDefaults:
  name: group-du-sno-pb
policyDefaults:
  #categories: []
  #controls:
  #  - PR.DS-1 Data-at-rest
  namespace: ztp-group
  # Use an existing placement rule so that placement bindings can be consolidated.
  placement:
    labelSelector:
      group-du-sno-zone: "zone-1"
      hardware-type: "hw-type-platform-1"
  remediationAction: inform
  severity: low
  # standards: []
  namespaceSelector:
    exclude:
      - kube-*
    include:
      - '*'
  evaluationInterval:
    compliant: 10m
    noncompliant: 10s
policies:
- name: gr-du-sno-cfg-pc-templated
  policyAnnotations:
    ran.openshift.io/ztp-deploy-wave: "10"
  manifests:
    - path: source-crs/DisableOLMPprof.yaml
    - path: source-crs/ClusterLogForwarder.yaml
      patches:
      - spec:
          filters:
          - name: ran-du-labels
            openshiftLabels: '{{hub fromConfigMap "" "group-zones-configmap" (printf "%s-cluster-log-fwd-filters-labels" (index .ManagedClusterLabels "group-du-sno-zone")) | toLiteral hub}}'
          outputs:
          - kafka:
              # below url is an example
              url: '{{hub fromConfigMap "" "group-zones-configmap" (printf "%s-cluster-log-fwd-outputs-url" (index .ManagedClusterLabels "group-du-sno-zone")) | toLiteral hub}}'
            # The name is needed for a correct merge with the source-cr.
            name: kafka-output
      openapi:
        path: schema.openapi
    ## The setting below overrides the default "worker" selector predefined in
    ## the source-crs. The change is recommended on SNOs configured with PTP 
    ## event notification for forward compatibility with possible SNO expansion.
    ## When the default setting is left intact, then in case of an SNO 
    ## expansion with one or more workers, PTP operator
    ## would not create linuxptp-daemon containers on the worker node(s). Any
    ## attempt to change the daemonsetNodeSelector will result in ptp daemon
    ## restart and time synchronization loss.
    ## After complying with the policy, complianceType can be set to a safer "musthave"
#    - path: source-crs/PtpOperatorConfigForEvent.yaml
#      complianceType: "mustonlyhave"
#      patches:
#      - spec:
#          daemonNodeSelector:
#            node-role.kubernetes.io/worker: ""
    - path: source-crs/PtpConfigSlave.yaml
      patches:
        - metadata:
            name: du-ptp-slave
          spec:
            profile:
                - interface: '{{hub fromConfigMap "" "group-hardware-types-configmap" (printf "%s-ptpcfgslave-profile-interface" (index .ManagedClusterLabels "hardware-type")) hub}}'
                  name: slave
                  phc2sysOpts: -a -r -n 24
                  ptp4lOpts: -2 -s --summary_interval -4
      openapi:
        path: schema.openapi
    - path: source-crs/SriovOperatorConfig-SetSelector.yaml
      complianceType: musthave
      patches:
      - spec:
          configDaemonNodeSelector:
            node-role.kubernetes.io/worker: ""
    - path: source-crs/StorageLV.yaml
      patches:
      - spec:
          storageClassDevices:
          - storageClassName: "example-storage-class-1"
            volumeMode: Filesystem
            fsType: xfs
            devicePaths: '{{hub fromConfigMap "" "group-hardware-types-configmap" (printf "%s-storagelv-devicepaths-1" (index .ManagedClusterLabels "hardware-type")) | toLiteral hub}}'
          - storageClassName: "example-storage-class-2"
            volumeMode: Filesystem
            fsType: xfs
            devicePaths: '{{hub fromConfigMap "" "group-hardware-types-configmap" (printf "%s-storagelv-devicepaths-2" (index .ManagedClusterLabels "hardware-type")) | toLiteral hub}}'
       ## This is required if PTP and BMER operators use HTTP transport.
       ## The disk labels are created by ignitionConfigOverride in siteConfig.
       ## - storageClassName: "storage-class-http-events"
       ##   volumeMode: Filesystem
       ##   fsType: xfs
       ##   devicePaths:
       ##   - /dev/disk/by-partlabel/httpevent1
       ##   - /dev/disk/by-partlabel/httpevent2
    - path: source-crs/DisableSnoNetworkDiag.yaml
    - path: source-crs/PerformanceProfile-SetSelector.yaml
      patches:
      - metadata:
          name: openshift-node-performance-profile
        spec:
          additionalKernelArgs:
          - rcupdate.rcu_normal_after_boot=0
          - vfio_pci.enable_sriov=1
          - vfio_pci.disable_idle_d3=1
          - efi=runtime
          cpu:
            # These must be tailored for the specific hardware platform
            isolated: '{{hub fromConfigMap "" "group-hardware-types-configmap" (printf "%s-cpu-isolated" (index .ManagedClusterLabels "hardware-type")) hub}}'
            reserved: '{{hub fromConfigMap "" "group-hardware-types-configmap" (printf "%s-cpu-reserved" (index .ManagedClusterLabels "hardware-type")) hub}}'
          hugepages:
            defaultHugepagesSize: '{{hub fromConfigMap "" "group-hardware-types-configmap" (printf "%s-hugepages-default" (index .ManagedClusterLabels "hardware-type")) hub}}'
            pages:
              - size: '{{hub fromConfigMap "" "group-hardware-types-configmap" (printf "%s-hugepages-size" (index .ManagedClusterLabels "hardware-type")) hub}}'
                count: '{{hub fromConfigMap "" "group-hardware-types-configmap" (printf "%s-hugepages-count" (index .ManagedClusterLabels "hardware-type")) | toInt hub}}'
          realTimeKernel:
            enabled: true
          machineConfigPoolSelector:
            pools.operator.machineconfiguration.openshift.io/master: ""
          nodeSelector:
            node-role.kubernetes.io/master: ''
    - path: source-crs/TunedPerformancePatch.yaml # wave 10
      patches:
      - spec:
          recommend:
          - machineConfigLabels:
              machineconfiguration.openshift.io/role: "master"
            priority: 19
            profile: performance-patch
    - path: source-crs/optional-extra-manifest/enable-crun-master.yaml
    - path: source-crs/optional-extra-manifest/enable-crun-worker.yaml
#    - path: source-crs/AmqInstance.yaml
#    - path: source-crs/StorageClass.yaml
#      patches:
#      - metadata:
#          name: image-registry-sc # FIXED
#    - path: source-crs/StoragePVC.yaml # wave 10
#      patches:
#      - metadata:
#          name: image-registry-pvc # FIXED
#          namespace: openshift-image-registry
#        spec:
#          accessModes:
#            - ReadWriteMany
#          resources:
#            requests:
#              storage: '{{hub fromConfigMap "" "group-zones-configmap" (printf "%s-storagepvc-storage" (index .ManagedClusterLabels "group-du-sno-zone")) hub}}'
#          storageClassName: image-registry-sc # FIXED
#          volumeMode: Filesystem # FIXED
#    - path: source-crs/ImageRegistryPV.yaml
#    - path: source-crs/ImageRegistryConfig.yaml
#      patches:
#      - spec:
#          storage:
#            pvc:
#              claim: "image-registry-pvc" # FIXED
    - path: source-crs/SriovFecClusterConfig.yaml
      patches:
      - metadata:
          name: fec-config
        spec:
          drainSkip: true
          acceleratorSelector:
            pciAddress: '{{hub fromConfigMap "" "group-hardware-types-configmap" (printf "%s-sriov-fec-pciAddress" (index .ManagedClusterLabels "hardware-type")) hub}}'
          physicalFunction:
            pfDriver: '{{hub fromConfigMap "" "group-hardware-types-configmap" (printf "%s-sriov-fec-pfDriver" (index .ManagedClusterLabels "hardware-type")) hub}}'
            vfDriver: '{{hub fromConfigMap "" "group-hardware-types-configmap" (printf "%s-sriov-fec-vfDriver" (index .ManagedClusterLabels "hardware-type")) hub}}'
            vfAmount: 16
            $patch: replace
- name: gr-du-sno-sriov-pc-templated
  policyAnnotations:
    ran.openshift.io/ztp-deploy-wave: "10"
  manifests:
    - path: source-crs/SriovNetwork.yaml # wave 100
      patches:
      - metadata:
          name: sriov-nw-du-fh # FIXED
        spec:
          resourceName: du_fh # FIXED
          vlan: '{{hub fromConfigMap "" "site-data-configmap" (printf "%s-sriov-network-vlan-1" .ManagedClusterName) | toInt hub}}'
    - path: source-crs/SriovNetworkNodePolicy-SetSelector.yaml # wave 100
      patches:
      - metadata:
          name: sriov-nnp-du-fh
        spec:
          deviceType: netdevice # FIXED
          isRdma: false
          nicSelector:
            pfNames: '{{hub fromConfigMap "" "group-hardware-types-configmap" (printf "%s-sriov-node-policy-pfNames-1" (index .ManagedClusterLabels "hardware-type")) | toLiteral hub}}'
          numVfs: 8
          priority: 10
          resourceName: du_fh
          nodeSelector:
            node-role.kubernetes.io/master: ""
    - path: source-crs/SriovNetwork.yaml # wave 100
      patches:
      - metadata:
          name: sriov-nw-du-mh # FIXED
        spec:
          resourceName: du_mh # FIXED
          vlan: '{{hub fromConfigMap "" "site-data-configmap" (printf "%s-sriov-network-vlan-2" .ManagedClusterName) | toInt hub}}'
    - path: source-crs/SriovNetworkNodePolicy-SetSelector.yaml # wave 100
      patches:
      - metadata:
          name: sriov-nw-du-fh
        spec:
          deviceType: netdevice # FIXED
          isRdma: false
          nicSelector:
            pfNames: '{{hub fromConfigMap "" "group-hardware-types-configmap" (printf "%s-sriov-node-policy-pfNames-2" (index .ManagedClusterLabels "hardware-type")) | toLiteral hub}}'
          numVfs: 8
          priority: 10
          resourceName: du_fh
          nodeSelector:
            node-role.kubernetes.io/master: ""
#   --- sources needed for updating CRI-O workload-partitioning ----
#    - path: source-crs/MachineConfigGeneric.yaml
#      complianceType: mustonlyhave # This is to update array entry as opposed to appending a new entry.
#      patches:
#      - metadata:
#          name: 02-master-workload-partitioning
#        spec:
#          config:
#            storage:
#              files:
#                - contents:
#                    # crio cpuset config goes below. This value needs to be updated and matched with PerformanceProfile. Check the link for more info on the content.
#                    source: '{{hub fromConfigMap "" "group-hardware-types-configmap" (printf "%s-machine-config-storage-source-1" (index .ManagedClusterLabels "hardware-type")) hub}}'
#                  mode: 420
#                  overwrite: true
#                  path: /etc/crio/crio.conf.d/01-workload-partitioning
#                  user:
#                    name: root
#                - contents:
#                    # openshift cpuset config goes below. This value needs to be updated and matched with crio cpuset (array entry above this). Check the link for more info on the content.
#                    source: '{{hub fromConfigMap "" "group-hardware-types-configmap" (printf "%s-machine-config-storage-source-2" (index .ManagedClusterLabels "hardware-type")) hub}}'
#                  mode: 420
#                  overwrite: true
#                  path: /etc/kubernetes/openshift-workload-pinning
#                  user:
#                    name: root
# --- START of source CRs needed for configuring OADP operator for SNO Image Based Upgrade ---
# - name: oadp-config-policy
#   policyAnnotations:
#     ran.openshift.io/ztp-deploy-wave: "100"
#   manifests:
#     - path: source-crs/OadpSecret.yaml
#       patches:
#       - data:
#           cloud: W2RlZmF1bHRdCmF3c19hY2Nlc3Nfa2V5X2lkPVdicktaSFpFOXZGWEVFemo2RU12CmF3c19zZWNyZXRfYWNjZXNzX2tleT1RRDNmRVZMNzVsOWJpSWswYW9PdlRSc2diN01ZRUlnZmF5bzVzRnlmCg==
#     - path: source-crs/OadpDataProtectionApplication.yaml
#       patches:
#       - spec:
#           backupLocations:
#           - velero:
#               provider: aws
#               default: true
#               credential:
#                 key: cloud
#                 name: cloud-credentials
#               config:
#                 profile: "default"
#                 region: minio
#                 s3Url: http://s3storage.example.com:9000
#                 insecureSkipTLSVerify: "true"
#                 s3ForcePathStyle: "true"
#               objectStorage:
#                 bucket: ibu
#                 prefix: '{{hub .ManagedClusterName hub}}'
#     - path: source-crs/OadpBackupStorageLocationStatus.yaml
#   --- END of source CRs needed for configuring OADP operator for SNO Image Based Upgrade ---
