# For this PGT, we've considered a 3 node managed cluster with the following labels:
#   group-du-3nc-zone: zone-1
#   hardware-type: hw-type-platform-1
# ConfigMaps used:
#   group-hardware-types-configmap.yaml: group-hardware-types-configmap
apiVersion: policy.open-cluster-management.io/v1
kind: PolicyGenerator
metadata:
  name: group-du-3nc-pg
placementBindingDefaults:
  name: group-du-3nc-pb
policyDefaults:
  # categories: []
  # controls:
  #   - PR.DS-1 Data-at-rest
  namespace: ztp-group
  # Use an existing placement rule so that placement bindings can be consolidated
  placement:
    labelSelector:
      group-du-3nc-zone: "zone-1"
      hardware-type: "hw-type-platform-1"
  remediationAction: inform
  severity: low
  # standards: []
  namespaceSelector:
    exclude:
      - kube-*
    include:
      - '*'
  evaluationInterval:
    compliant: 10m
    noncompliant: 10s
policies:
- name: group-du-3nc-cfg-pc
  policyAnnotations:
    ran.openshift.io/ztp-deploy-wave: "10"
  manifests:
    - path: source-crs/PtpConfigSlave.yaml   # Change to PtpConfigSlaveCvl.yaml for ColumbiaVille NIC
      patches:
        - metadata:
            name: du-ptp-slave
          spec:
            profile:
                - interface: '{{hub fromConfigMap "" "group-hardware-types-configmap" (printf "%s-ptpcfgslave-profile-interface" (index .ManagedClusterLabels "hardware-type")) hub}}'
                  name: slave
                  phc2sysOpts: -a -r -n 24
                  ptp4lOpts: -2 -s --summary_interval -4
      openapi:
        path: schema.openapi
    - path: source-crs/SriovOperatorConfig-SetSelector.yaml
      # For existing clusters with node selector set as "master", 
      # change the complianceType to "mustonlyhave".
      # After complying with the policy, the complianceType can 
      # be reverted to "musthave"
      complianceType: musthave
      patches:
      - spec:
          configDaemonNodeSelector:
            node-role.kubernetes.io/master: ""
    - path: source-crs/PerformanceProfile-SetSelector.yaml
      patches:
      - spec:
          cpu:
            # These must be tailored for the specific hardware platform
            isolated: '{{hub fromConfigMap "" "group-hardware-types-configmap" (printf "%s-cpu-isolated" (index .ManagedClusterLabels "hardware-type")) hub}}'
            reserved: '{{hub fromConfigMap "" "group-hardware-types-configmap" (printf "%s-cpu-reserved" (index .ManagedClusterLabels "hardware-type")) hub}}'
          hugepages:
            defaultHugepagesSize: '{{hub fromConfigMap "" "group-hardware-types-configmap" (printf "%s-hugepages-default" (index .ManagedClusterLabels "hardware-type")) hub}}'
            pages:
            - size: '{{hub fromConfigMap "" "group-hardware-types-configmap" (printf "%s-hugepages-size" (index .ManagedClusterLabels "hardware-type")) hub}}'
              count: '{{hub fromConfigMap "" "group-hardware-types-configmap" (printf "%s-hugepages-count" (index .ManagedClusterLabels "hardware-type")) | toInt hub}}'
          machineConfigPoolSelector:
            pools.operator.machineconfiguration.openshift.io/master: ""
          nodeSelector:
            node-role.kubernetes.io/master: ''
    - path: source-crs/TunedPerformancePatch.yaml
      patches:
      - spec:
          recommend:
          - machineConfigLabels:
              machineconfiguration.openshift.io/role: "master"
            priority: 19
            profile: performance-patch
    - path: source-crs/optional-extra-manifest/enable-crun-master.yaml
    - path: source-crs/optional-extra-manifest/enable-crun-worker.yaml
